export type MindLLMFinishReason = 'stop' | 'length';

export interface MindLLMGenerateOptions {
  maxTokens?: number;
  temperature?: number;
  stop?: string[];
  systemPrompt?: string;
  metadata?: Record<string, unknown>;
}

export interface MindLLMGenerateResult {
  text: string;
  tokens: number;
  finishReason: MindLLMFinishReason;
  metadata?: Record<string, unknown>;
}

/**
 * LLM Engine interface for Mind v2.
 * 
 * Supports:
 * - OpenAI models (via createOpenAILLMEngine)
 * - Local stub for development (via createLocalStubLLMEngine)
 * - Future: Local models (Ollama, llama.cpp, etc.)
 * 
 * The interface is language-agnostic and works with any LLM provider
 * that implements this contract.
 */
export interface MindLLMEngine {
  readonly id: string;
  readonly description?: string;
  generate(
    prompt: string,
    options?: MindLLMGenerateOptions,
  ): Promise<MindLLMGenerateResult>;
}

export interface LocalStubEngineOptions {
  id?: string;
  maxEchoLines?: number;
}

/**
 * Minimal local engine placeholder used for offline development.
 * Produces deterministic responses derived from prompt contents.
 */
export function createLocalStubLLMEngine(
  options: LocalStubEngineOptions = {},
): MindLLMEngine {
  const engineId = options.id ?? 'mind-local-llm-stub';
  const maxLines = options.maxEchoLines ?? 8;

  return {
    id: engineId,
    description: 'Local stub LLM engine (deterministic echo).',
    async generate(prompt, generateOptions) {
      const lines = prompt
        .split(/\r?\n/)
        .map(line => line.trim())
        .filter(Boolean)
        .slice(0, maxLines);
      const body = lines.join('\n');
      const suffix =
        generateOptions?.metadata && Object.keys(generateOptions.metadata).length
          ? `\n\n[meta:${Object.keys(generateOptions.metadata).join(',')}]`
          : '';
      const response = `${body}\n\n[stub-response generated by ${engineId}]${suffix}`;
      const maxTokens = generateOptions?.maxTokens ?? 256;
      const trimmed = response.slice(0, maxTokens);
      return {
        text: trimmed,
        tokens: Math.min(response.length, maxTokens),
        finishReason: trimmed.length < response.length ? 'length' : 'stop',
      };
    },
  };
}

// Export OpenAI engine
export { createOpenAILLMEngine } from './openai.js';
export type { OpenAILLMEngineOptions } from './openai.js';
